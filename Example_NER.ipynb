{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipymarkup import show_box_markup\n",
    "from ipymarkup.palette import palette, BLUE, RED, GREEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rusenttokenize import ru_sent_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def txt_to_span(file):\n",
    "    text, spans = '', [] \n",
    "    lines = [i[:-1] for i in open(file, 'r').readlines()]\n",
    "    count_s = 0\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            #print(line)\n",
    "            word, tag = line.split(' ')[0], line.split(' ')[1]\n",
    "            if tag!='O':\n",
    "                spans.append((count_s, count_s +len(word), tag))\n",
    "                count_s+=len(word)+1\n",
    "                text+=word + ' '\n",
    "            else:\n",
    "                text+=word + ' '\n",
    "                count_s+=len(word)+1\n",
    "        else:\n",
    "            text+='\\n'\n",
    "            count_s+=1\n",
    "    return text, spans\n",
    "\n",
    "\n",
    "def text_to_model(text):\n",
    "    testfile = open('./model_1/test.txt', 'w')\n",
    "    start = 0\n",
    "    labels = []\n",
    "    words = []\n",
    "    for sentence in ru_sent_tokenize(text):\n",
    "        if sentence:\n",
    "            words = wordpunct_tokenize(sentence)\n",
    "            labels = (['O'] * len(words))\n",
    "\n",
    "            assert len(labels) == len(words), 'Mismatch len labels ans len sentence'\n",
    "\n",
    "            for word, tag in zip(words, labels):\n",
    "                testfile.write(word + ' ' + tag + '\\n')\n",
    "            testfile.write('\\n')\n",
    "    testfile.close()\n",
    "    !python3 run_ner.py --data_dir ./model_1 --model_type bert --model_name_or_path model_1 --overwrite_cache --config_name model_1 --labels labels_.txt --tokenizer_name model_1/vocab.txt  --do_predict --output_dir ./model_1\n",
    "    return txt_to_span('model_1/test_predictions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/04/2020 17:17:38 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "03/04/2020 17:17:38 - INFO - transformers.configuration_utils -   loading configuration file model_1/config.json\n",
      "03/04/2020 17:17:38 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LOC\",\n",
      "    \"1\": \"PER\",\n",
      "    \"2\": \"ORG\",\n",
      "    \"3\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LOC\": 0,\n",
      "    \"O\": 3,\n",
      "    \"ORG\": 2,\n",
      "    \"PER\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "03/04/2020 17:17:38 - INFO - __main__ -   Tokenizer arguments: {'do_lower_case': False}\n",
      "03/04/2020 17:17:38 - INFO - transformers.tokenization_utils -   Model name 'model_1/vocab.txt' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'model_1/vocab.txt' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/04/2020 17:17:38 - WARNING - transformers.tokenization_utils -   Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n",
      "03/04/2020 17:17:38 - INFO - transformers.tokenization_utils -   loading file model_1/vocab.txt\n",
      "03/04/2020 17:17:38 - INFO - transformers.modeling_utils -   loading weights file model_1/pytorch_model.bin\n",
      "03/04/2020 17:17:41 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='model_1', data_dir='./model_1', device=device(type='cpu'), do_eval=False, do_lower_case=False, do_predict=True, do_train=False, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, keep_accents=None, labels='labels_.txt', learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=100, model_name_or_path='model_1', model_type='bert', n_gpu=0, no_cuda=False, num_train_epochs=3.0, output_dir='./model_1', overwrite_cache=True, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=500, seed=42, server_ip='', server_port='', strip_accents=None, tokenizer_name='model_1/vocab.txt', use_fast=None, warmup_steps=0, weight_decay=0.0)\n",
      "03/04/2020 17:17:41 - INFO - transformers.tokenization_utils -   Model name './model_1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming './model_1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/04/2020 17:17:41 - INFO - transformers.tokenization_utils -   Didn't find file ./model_1/added_tokens.json. We won't load it.\n",
      "03/04/2020 17:17:41 - INFO - transformers.tokenization_utils -   loading file ./model_1/vocab.txt\n",
      "03/04/2020 17:17:41 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/04/2020 17:17:41 - INFO - transformers.tokenization_utils -   loading file ./model_1/special_tokens_map.json\n",
      "03/04/2020 17:17:41 - INFO - transformers.tokenization_utils -   loading file ./model_1/tokenizer_config.json\n",
      "03/04/2020 17:17:42 - INFO - transformers.configuration_utils -   loading configuration file ./model_1/config.json\n",
      "03/04/2020 17:17:42 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LOC\",\n",
      "    \"1\": \"PER\",\n",
      "    \"2\": \"ORG\",\n",
      "    \"3\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LOC\": 0,\n",
      "    \"O\": 3,\n",
      "    \"ORG\": 2,\n",
      "    \"PER\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "03/04/2020 17:17:42 - INFO - transformers.modeling_utils -   loading weights file ./model_1/pytorch_model.bin\n",
      "03/04/2020 17:17:45 - INFO - __main__ -   Creating features from dataset file at ./model_1\n",
      "03/04/2020 17:17:45 - INFO - utils_ner -   Writing example 0 of 1\n",
      "03/04/2020 17:17:45 - INFO - utils_ner -   *** Example ***\n",
      "03/04/2020 17:17:45 - INFO - utils_ner -   guid: test-1\n",
      "03/04/2020 17:17:45 - INFO - utils_ner -   tokens: [CLS] Я еду к себе домой в город Москва . [SEP]\n",
      "03/04/2020 17:17:45 - INFO - utils_ner -   input_ids: 101 839 47344 861 10379 18829 845 3106 11135 132 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/04/2020 17:17:45 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/04/2020 17:17:45 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "03/04/2020 17:17:45 - INFO - utils_ner -   label_ids: -100 3 3 3 3 3 3 3 3 3 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/logging/__init__.py\", line 1034, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/logging/__init__.py\", line 880, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/logging/__init__.py\", line 619, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/logging/__init__.py\", line 380, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"run_ner.py\", line 695, in <module>\n",
      "    main()\n",
      "  File \"run_ner.py\", line 669, in main\n",
      "    result, predictions = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=\"test\")\n",
      "  File \"run_ner.py\", line 265, in evaluate\n",
      "    eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode)\n",
      "  File \"run_ner.py\", line 370, in load_and_cache_examples\n",
      "    pad_token_label_id=pad_token_label_id,\n",
      "  File \"/Users/a18357051/Documents/Projects/NER_bert/utils_ner.py\", line 200, in convert_examples_to_features\n",
      "    logger.info( str(broken) , 'broken sentences were found')\n",
      "Message: '0'\n",
      "Arguments: ('broken sentences were found',)\n",
      "03/04/2020 17:17:45 - INFO - __main__ -   Saving features into cached file ./model_1/cached_test_model_1_128\n",
      "03/04/2020 17:17:45 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "03/04/2020 17:17:45 - INFO - __main__ -     Num examples = 1\n",
      "03/04/2020 17:17:45 - INFO - __main__ -     Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                         | 0/1 [00:00<?, ?it/s]\r",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  9.08it/s]\r",
      "Evaluating: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  9.06it/s]\r\n",
      "03/04/2020 17:17:45 - INFO - __main__ -   ***** Eval results  *****\r\n",
      "03/04/2020 17:17:45 - INFO - __main__ -     f1 = 0\r\n",
      "03/04/2020 17:17:45 - INFO - __main__ -     loss = 0.018412873148918152\r\n",
      "03/04/2020 17:17:45 - INFO - __main__ -     precision = 0\r\n",
      "03/04/2020 17:17:45 - INFO - __main__ -     recall = 0\r\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Я еду к себе домой в город Москва.'\n",
    "\n",
    "text, spans = text_to_model(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tex2jax_ignore\" style=\"white-space: pre-wrap\">Я еду к себе домой в город Москва . \n",
       "</div>"
      ],
      "text/plain": [
       "BoxMarkup('Я еду к себе домой в город Москва . \\n', [])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_box_markup(text, spans, palette=palette(PER=BLUE, ORG=RED, LOC=GREEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
